{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# just for the sake of this blog post!\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../dataset'\n",
    "X = pd.read_csv(filepath + '/dengue_features_train.csv')\n",
    "Y = pd.read_csv(filepath + '/dengue_labels_train.csv')\n",
    "T = pd.read_csv(filepath + '/dengue_features_test.csv')\n",
    "\n",
    "# concating total cases to train data frame - beacuse of issue in removing outliers\n",
    "X = pd.concat([Y['total_cases'], X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot heat map of dataset\n",
    "def plotHeatMap(dataset):\n",
    "    f,ax = plt.subplots(figsize=(18, 18))\n",
    "    sn.heatmap(dataset.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove correlated columns\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if np.absolute(corr_matrix.iloc[i, j]) >= threshold:\n",
    "                colname = corr_matrix.columns[i] # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "                if colname in dataset.columns:\n",
    "                    del dataset[colname] # deleting the column from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop columns having more than 10% NaN values\n",
    "NaNDic = (X.isnull().sum()*100/X.shape[0])>=10\n",
    "for i in X.columns.values:\n",
    "    if(NaNDic[i]):\n",
    "        X.drop(i,axis=1,inplace=True)\n",
    "        T.drop(i,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seperate into two cities\n",
    "X_sj = X[X['city'] == \"sj\"]\n",
    "X_iq = X[X['city'] == \"iq\"]\n",
    "T_sj = T[T['city'] == \"sj\"]\n",
    "T_iq = T[T['city'] == \"iq\"]\n",
    "# drop columns\n",
    "dropping_columns = ['city']\n",
    "X_sj = X_sj.drop(dropping_columns, axis=1)\n",
    "X_iq = X_iq.drop(dropping_columns, axis=1)\n",
    "T_sj = T_sj.drop(dropping_columns, axis=1)\n",
    "T_iq = T_iq.drop(dropping_columns, axis=1)\n",
    "# fill NaN values\n",
    "X_sj.interpolate(inplace=True)\n",
    "X_iq.interpolate(inplace=True)\n",
    "T_sj.interpolate(inplace=True)\n",
    "T_iq.interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "X_sj = X_sj[(np.abs(stats.zscore(X_sj.drop(['year','weekofyear','week_start_date','total_cases'],axis=1))) < 5).all(axis=1)]\n",
    "X_iq = X_iq[(np.abs(stats.zscore(X_iq.drop(['year','weekofyear','week_start_date','total_cases'],axis=1))) < 5).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sperating total_cases label again\n",
    "L_sj = pd.DataFrame(X_sj['total_cases'])\n",
    "L_iq = pd.DataFrame(X_iq['total_cases'])\n",
    "\n",
    "# drop total_cases and back X_sj,X_iq in dataset\n",
    "X_sj = X_sj.drop(['total_cases'],axis=1)\n",
    "X_iq = X_iq.drop(['total_cases'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concating test and train\n",
    "XandT_sj = pd.concat([X_sj, T_sj])\n",
    "XandT_iq = pd.concat([X_iq, T_iq])\n",
    "\n",
    "# stores droped columns from XandT_sj, XandT_iq\n",
    "XandT_sj_rest = pd.DataFrame(XandT_sj[['year','week_start_date']])\n",
    "XandT_iq_rest = pd.DataFrame(XandT_iq[['year','week_start_date']])\n",
    "\n",
    "#\n",
    "XandT_sj.drop(['year','week_start_date'], axis=1, inplace=True)\n",
    "XandT_iq.drop(['year','week_start_date'], axis=1, inplace=True)\n",
    "\n",
    "# scaling training set with test set together\n",
    "XandT_sj[XandT_sj.columns] = MinMaxScaler().fit_transform(XandT_sj)\n",
    "XandT_iq[XandT_iq.columns] = MinMaxScaler().fit_transform(XandT_iq)\n",
    "\n",
    "XandT_sj = pd.concat([XandT_sj_rest, XandT_sj], axis=1)\n",
    "XandT_iq = pd.concat([XandT_iq_rest, XandT_iq], axis=1)\n",
    "\n",
    "# final scaled X\n",
    "X_sj = XandT_sj[:926]\n",
    "X_iq = XandT_iq[:515]\n",
    "\n",
    "# final scaled T\n",
    "T_sj = XandT_sj[926:]\n",
    "T_iq = XandT_iq[515:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                                    -0.212506\n",
       "weekofyear                               0.288052\n",
       "ndvi_nw                                  0.042507\n",
       "ndvi_se                                 -0.046217\n",
       "ndvi_sw                                  0.040561\n",
       "precipitation_amt_mm                     0.075728\n",
       "reanalysis_air_temp_k                    0.180454\n",
       "reanalysis_avg_temp_k                    0.173806\n",
       "reanalysis_dew_point_temp_k              0.203985\n",
       "reanalysis_max_air_temp_k                0.193488\n",
       "reanalysis_min_air_temp_k                0.186487\n",
       "reanalysis_precip_amt_kg_per_m2          0.148030\n",
       "reanalysis_relative_humidity_percent     0.148630\n",
       "reanalysis_sat_precip_amt_mm             0.075728\n",
       "reanalysis_specific_humidity_g_per_kg    0.208200\n",
       "reanalysis_tdtr_k                       -0.066252\n",
       "station_avg_temp_c                       0.194726\n",
       "station_diur_temp_rng_c                  0.035631\n",
       "station_max_temp_c                       0.188667\n",
       "station_min_temp_c                       0.175596\n",
       "station_precip_mm                        0.064274\n",
       "Name: total_cases, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlations\n",
    "sj_correlations = pd.concat([X_sj, L_sj], axis=1).corr().total_cases.drop('total_cases')\n",
    "iq_correlations = pd.concat([X_iq, L_iq], axis=1).corr().total_cases.drop('total_cases')\n",
    "\n",
    "\n",
    "# low results\n",
    "for i in X_sj.drop(['year','week_start_date'], axis=1).columns.values:\n",
    "    X_sj[i] = X_sj[i] * np.absolute(sj_correlations[i]) * 100\n",
    "    X_iq[i] = X_iq[i] * np.absolute(iq_correlations[i]) * 100\n",
    "    \n",
    "X_sj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XTT = train_test_split(X, test_size=0.33, shuffle=False)\n",
    "# X_train = XTT[0]\n",
    "# X_test = XTT[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10000, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_model_sj = RandomForestRegressor(n_estimators=10000)\n",
    "forest_model_sj.fit(X_sj.drop(['week_start_date','year'], axis=1), L_sj)\n",
    "\n",
    "forest_model_iq = RandomForestRegressor(n_estimators=10000)\n",
    "forest_model_iq.fit(X_iq.drop(['week_start_date','year'], axis=1), L_iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_predict_sj = forest_model_sj.predict(T_sj.drop(['week_start_date','year'], axis=1))\n",
    "forest_predict_iq = forest_model_iq.predict(T_iq.drop(['week_start_date','year'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_list = list((forest_predict_sj).astype(int)) + list((forest_predict_iq).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forest_predict_frame = pd.DataFrame((forest_predict).astype(int))\n",
    "S = pd.read_csv(filepath + '/submission_format.csv')\n",
    "\n",
    "S['total_cases'] = predict_list\n",
    "\n",
    "S\n",
    "\n",
    "S.to_csv(filepath + '/forest_test_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. fill NaN with random forest - feature with correlated columns\n",
    "2. weekofyear drop - month feature\n",
    "3. (correlation_value)^2 * column_value - xxxxxxxxxxxxxxxxxxx\n",
    "4. moving window - *************************\n",
    "5. shift\n",
    "6. weka - **********************\n",
    "7. changing the model\n",
    "8. total_cases correlation high features only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
